Concierto 
=========
An Application Orchestration Tool
:toc: left
:source-highlighter: highlight.js
:highlightjs-languages: clojure,bash,dockerfile,yaml,nginx
:doctype: book
:description: Concierto, an orchestration tool
:icons: font
:toc: left
:toclevels: 2
:toc-collapsable:
:sectlinks:

== Introduction

Concierto manages a top-level git repository where an entire application defined as docker services (in git sub-repos for non trivial services) can be built and deployed. All configuration and versioning is in one place for multiple deployment targets.

Concierto is:

- A command line orchestration tool utilising docker and docker-compose (or podman and podman-compose).
- A build tool for an application where an application is defined as multiple docker services.
- A handy tool for managing many machines.
- Good at deploying different versions of an application to different sets of machines. 
- Has a simple iteration and templating model with no magic or daemons, simply ssh from control to managed nodes.
- The Spanish spelling of concerto, cos i was able to get conciert.io - which still doesn't work, but hey ...

_This documentation assumes familiarity with docker, and docker-compose files, some scripting and a knowledge of Clojure for extensions._

There is a video tutorial https://www.conciert.io/starter.html[here] with LXD clusters.

Github https://github.com/conciertio[here]

=== Motivation

I needed a tool that could be handed off to customers to build/deploy services, add machines and provide a complete overview of a system from a git repository with sub-repos. This had to be done by knowledgeable stakeholders, but not necessarily programmers or devops, that is, i could setup a system then provide this tool as an acceptance test and means of self management, as i stepped away.

For more info mail contact@conciert.io

[IMPORTANT]
At time of writing, I haven't frozen or documented any notion of a public api to be called by extensions or if I have extensions right (input welcome), until I do I'd say the project is in *late alpha*. 


=== Implementation

Concierto:

- Is written in http://www.babashka.org[Babashka], a fast starting dialect of Clojure. 
- Functions nicely on Mac and Linux clients. Although babashka functions on Windows, Concierto has not been tested there.
- May be extended via Clojure.

_I haven't tested it on the Mac for about a month so there may be issues I'm unaware of, but if there are they should be trivial._


=== Usage

The typical usage is to install the bootstrap "concierto" program in a bin dir and call ...

[literal]
----
concierto
include::help.adoc[]
----

[NOTE]
The extensions in the listing above come with the starter pack.

=== Flavour of Commands

Concierto provides the following functionality.

=== Configuration Settings

    concierto set scenario tiered
    concierto set target live
    concierto set engine podman

These are stored in concierto.conf.

==== List some machines

    concierto machines --list ip --select role api cluster west
    10.174.25.155
    10.174.25.37
    10.174.25.195

or json ...

    concierto machines --json --select role db
    [{"role":"db","cluster":"west","ip":"10.174.25.250","host":"west-db","added":"dynamically"}]

or the default edn ...

    concierto machines --select role api
    ({:role :api, :cluster "west", :ip "10.174.25.155", :host "west-api1", :added "dynamically"}
    {:role :api, :cluster "west", :ip "10.174.25.37", :host "west-api2", :added "dynamically"}
    {:role :api, :cluster "west", :ip "10.174.25.195", :host "west-api3", :added "dynamically"})


==== Build and deploy

Build all services in order based on sequence in services/build.edn, then deploy 3 tiers of service to live servers.

    concierto build all --push --target live
    concierto deploy \
        --scenario tiered \
        --target live \
        --select \ 
            role db also \
            role api also \
            role balancer 

==== Arbitrary ssh

    concierto ssh --target live --select role db -- uptime
    east-db: 12:52:39 up 1 day,  4:44,  0 users,  load average: 0.79, 0.72, 0.70
    west-db: 12:52:39 up 1 day,  4:44,  0 users,  load average: 0.79, 0.72, 0.70

==== Remote Scripts

Upload and execute scripts, e.g. for a script file, defined in scripts/docker-login

[source, bash]
----
#!/usr/bin/env sh

podman login {{access.docker.registry.url}}:5000 \
     -u {{access.docker.registry.user}} -p {{access.docker.registry.secret}}
----

Run it remotely, but with pre upload templating per target ...

[source, bash]
----
concierto rexec --target live --select cluster west -- scripts/docker-login
----

=== Template any File

[literal]
----
echo "{% for m in machines %}
            {{m.host}} {% endfor %>}" > host.tmpl
    concierto template --select cluster west --in=host.tmpl -q

        west-api1 
        west-api2 
        west-api3 
        west-balancer 
        west-db 
----

== Application Structure

Concierto is mostly a declarative tool where the layout of directories and files defines your cloud application; Concierto ties the various files together with Django inspired https://github.com/yogthos/Selmer[templating]. 

Concierto revolves around the idea of *services* - code deployed as containers.

Concierto defines a machine *role* as a docker compose file. That is, if docker containers are instantiated with docker-compose X.yml then that machine fullfills role X. Machines are tagged with at least 1 role.

Roles are aggregated as *scenarios*. For example:

* a test scenario could apply machine limits; memory, cpu etc to running services, whereas a production scenario may be unlimited.
* bronze, silver, gold service level scenarios 
* tiered roles over many machines, as opposed to a single machine and role (developer)

Scenarios allow flexibily switching sets of  docker-compose files, favouring declarative verbosity over embedding template logic for multiple variation.

*targets* assemble and aggregate all data that will be subsequently used in the build/deploy or any other process that could be implemented by extensions. 

For each concept; service, scenario and target there is a corresponding directory for specification. 

Finally, Concierto may be extended with Clojure within the extensions directory, or by hooks via the hooks.clj file in the targets directory.

The typical structure is:

[literal]
----
include::tree.adoc[]
----

=== Target Files

As you can see above the target is defined by some key files ...

[cols="1,3"]
|===
| access (or .clj/.edn)
| A map with ssh credentials and registry details

|globals (or .clj/.edn)
| A map, general info to be substituted as ENV vars.

|machines (or .clj/.edn)
|A sequence of machines, with their IP address, role, hostname and cluster, and any other searchable attributes.

|services (or .clj/.edn)
|A map of service images/versions for the target.

|hooks.clj
|A Clojure file with functions that are executed at critical moments during a deploy or other process. More info here <<_hooks>>
|===

If a file without a .clj or .edn extension is supplied it is run as a #! function, and must write the edn data format to stdout.

If a clj file is used it must supply a "source" function which returns a map or sequence as required.

These files will be described in <<_details>>, but the basic functioning of Concierto is to aggregate all data generated from these files into a map (keyed by filename) for a given target and then use the data for templating during any subsequent process.

=== What's Available for Templating?

The *gather* command provides the aggregation of data from the target files, including dynamic attributes created for clusters and machines. Thus, *gather* shows you exactly what's available to work with in a template.

    concierto gather --select role db also role balancer

[source, clojure]
----
include::attr.adoc[]
----

Attribute *:machines* is different from the raw machines.edn. It's a sequence of possible groups, selected by *also* clause. The default is a sequence of one group, multiple *also* will provide a sequence of sequences. Further, the dynamic attributes have been applied (see <<_hooks>>)

Attribute *:clusters* is created by the hook function attr-cluster (see <<_hooks>> for details).

[IMPORTANT]
During deploy and hence within the role files, 2 further attributes are added, :machine and :cluster. Although all the information is calculated prior to deploy (exactly as the gather command displays), the looping through the machines per deploy requires each instance of :machine and :cluster detail to be added dynamically.

== Details

=== Services

A service is the code you wish to deploy in a running container. All services in your app are defined in subdirectories of the top level 'services' directory. 

The meta data, however, is defined in services.edn *per target*. Here is a typical services.edn ...

==== services.edn

[source,clojure]
----
{:alpine {:image "alpine:3.18"}
 :python {:image "mypy:1.0.0"}
 :db {:image "mydb:1.0.0"}
 :api {:image "myapi:1.0.0"}
 :balancer {:image "mybalancer:1.0.0"}}
----

Thus each target can define different image versions, e.g. a live target will not run the same image versions as a stage target as staging by definition is the future of the app.

[IMPORTANT]
The service key, e.g. :db, is very important. The service key in the role files, and the service directory names must match the service key given in services.edn. The service keys tie the elements of the system together.

==== Substitution example services.edn

You may now reference the services.edn definition from a Dockerfile (e.g. in services/db/Dockerfile)

[source, Dockerfile]
----
    FROM: {{services.alpine.image}}
    RUN apk add redis
----

and calls to build, e.g.

    concierto build db

will substitute alpine:3.18 appropriately as it builds mydb:1.0.0 locally.

See <<_services_2>> instructions for details on building and pushing docker images.

[NOTE]
Although Docker build has it's own substitution mechanism, Concierto performs it's substitutions pre-build, and attempts to provide the {{}} templating in every possible situation and doesn't detract from existing forms when you wish to use them.

==== Non Standard Templating?

One potential negative is the fact your Dockerfiles have non-standard templating and you can't build them independently. This can be solved by templating your Dockerfiles on the fly. e.g. ...

[source]
----
concierto template  --in services/db/Dockerfile
FROM alpine:3.18

RUN apk add redis
ADD runner /runner
CMD [ "/runner" ]
----

Just redirect to a new Dockerfile and build, script it or add an extension to render a non issue.

=== Targets

A deployment target contains the machines and variables required to deploy an application successfully, for example to a given customer.

The targets directory contains sub-directories for each deployment target; target details are specified in a minimum of 4 files; access, machines, services and globals (as described here <<_target_files>>)

Concierto will attempt to load a .clj file first (with a source function), fallback to a .edn file and if that does not exist the file name without extension is treated as a generic script. This allows you to get machines, globals or other resources declaratively or programmatically (from clojure or a #! script). A #! script must print valid edn to stdout.

The target data is related directly to templating via the top level file names:

services.edn data may be referenced in any template directly as 

    {{services.KEY}}

or globals.edn data as 
    
    {{globals.KEY}}

Additionally, you may specify any other number of resource files in the target as either maps or sequences that you wish to look up in templating, so otherglobs.edn could be accessed as 

    {{otherglobs.KEY}}

by specifying the --resource <list of resource files> key on the command line like this

    concierto deploy --select cluster stage --resource otherglobs

Concierto will look for otherglobs.clj falling back to otherglobs.edn and make the Clojure datastructure available in your templating.

[WARNING]
Each extra file you provide will increase load times.


==== machines.edn

Here's an example of a minimal machines.edn file, which as you can see is a sequence of maps

[source, clojure]
----
include::machines.edn[]
----

The minimum number of keys to specify a machine is as you can see; role, cluster, ip, host.

You may add any other attributes and they will be searchable from the --select clause on the command line.

[NOTE]
An :ssh key can can be defined to allow specific username/port combinations per machine overriding the target defined access details.

Here is a potential machines.clj file, where the machine list is coming from an extension, but it must return the minimum number of keys as specified above.

[source, clojure]
----
include::../../starter-pack/targets/live/machines.clj[]
----


==== globals.edn

Here is a typical globals.edn file ...

[source,clojure]
----
{:DB_PASSWORD "superduper"
 :DB_PORT 6379
 :API_PORT 8080
 :BALANCER_PORT 9000}
----

You may place any key in this file and reference it for any use, by using the file name "globals" as the top level key, e.g.

    {{globals.DB_PASSWORD}}


==== access.edn

Contains mandatory keys if you want to deploy, here is a typical file ...

[source, clojure]
----
{:ssh {:user "concierto" :port 22}
 :docker {:registry  {:url "10.174.25.1:5000"
                      :user "ritchie"
                      :secret "ritchie"}}
----

Here's an example of an "access" script (i.e. without clj or edn ). Notice a script must write valid edn to stdout (target scripts may not be templated as they are the source of templates).

[source, bash]
----
include::../../starter-pack/targets/live/access[]
----

[TIP]
Any file in the targets directory can be a #! script, just remove the extension and chmod +x.

===== Required Keys

[cols="1,~"]
|===
|:ssh
|:port and :user are both required, note, the global setting can be overwritten by the same keys per machine in the machines files.

|:docker 
|registry.url is required, other keys could be provided if you use them in other scripts, e.g. user,secret for a docker login
|===

==== The Config File

Each taget directory can also contain a config file, concierto.conf, to override specific keys ...

[cols="2,5"]
|===
|:remote-exec
|supply path to a remote execution script for a deploy for a target

|:engine
|Target can use specific container engine. 
|===



=== Scenarios and Roles

A scenario directory contains docker-compose yml files. Each yml file is known as a role, that is, a series of services which fullfill a function on a given machine. Each machine could have multiple roles (specified as a :role vector in machines.edn).

The yml files may be  https://github.com/yogthos/Selmer[templated] using attributes defined for the specified target.

[source,yaml]
----
version: "3.7"
services:
  db:
    image: "{{services.db.image|registry}}"
    container_name: "redis"
    environment:
      DB_PASSWORD: {{globals.DB_PASSWORD}}
    ports:
      - "{{globals.DB_PORT}}:6379"
----

The registry filter in {{services.db.image|registry}} adds the registry defined in access.edn to urls when the service is pushed to the registry and when role file is deployed. Otherwise for local usage it returns an empty string allowing use of the local image.

[IMPORTANT]
All of your compose file image specs should include the |registry filter.

[NOTE]
A machine may have multiple roles specified by including a vector of roles in the machine spec in machines.edn, e.g. for an infrastructure machine you could specify {:role [:git :registry] .... }


== Getting Started

=== Prerequisites

==== Babashka

The concierto and concierto.jar comprise the Concierto system, however they rely on a Clojure scripting environment called Babashka, Babashka is a single binary. To install it on Mac

    brew install borkdude/brew/babashka

To install on linux, as root

[source, bash]
----
curl -sLO https://raw.githubusercontent.com/babashka/babashka/master/install
chmod +x install
./install
----

Download concierto, putting it in a directory with an executable PATH that is convenient for your system, e.g. ~/.local/bin, and make it executable

    wget "https://www.conciert.io/release/concierto" -O ~/.local/bin/concierto
    chmod +x ~/.local/bin/concierto

In the directory you wish to make your concierto app dir, you may initalise the app dir with

    concierto update

The update command gets the latest Concierto jar file and puts it in ./concierto/concierto.jar. The notion here is that the jar file gets versioned with the rest of your app. The concierto command always runs the jar file from your application directory.

You should now be able to run concierto and get the command help.

==== Managed Nodes

- Should have docker/podman installed.
- Should have docker-compose, docker compose or podman-compose installed.
- Should be accessible over ssh by public key by the user, and the user login should be a member of the docker group.
- "docker login your-registry" should have been run on each node.

e.g. the starter-pack base image is setup with the following cloud-init spec ...

[source, yaml]
----
include::../../starter-pack/extensions/demo/lxd/cloud-init-podman.yml[]
----

[IMPORTANT]
Obviously, the setup above is tailored to your requirements. There's no need to use a "concierto" login etc.

==== Docker Registry
    
- Each deployment target is assumed to have a docker registry defined using the 'docker.registry' key in access.edn for a given target.

[TIP]
Check the https://www.conciert.io/starter.html[starter-pack] documentation for a video tutorial using LXD in the setup of a multiple cluster, service and tier application.

== Services

=== Build

Concierto assumes that you have images in a docker registry that you deploy from. If your service images already exist in a docker registry then you may deploy from that registry without a build process.

However, if you require a build process, Concierto can help with some convention.  Given the service "db", then the following directory layout is assumed

    services/
      db/
        Dockerfile

[TIP]
Remember the service name must match with name of a service in the targets service.edn file.

On calling

    concierto build db --target live

Concierto will build your Dockerfile, templating any variables defined from the templates's resource files. For example, typically your FROM directive in the Dockerfile will refer to an existing image in the target, e.g. lets say your base alpine image

    FROM: {{services.alpine.image}}

This is very useful as it keeps all your service versions versioned in a single file target/live/services.edn.

The docker image is built locally with the image name given in the services.edn file under, e.g. for service db

 :db {:image "mydb:1.0.0"}

=== Push

If your build script creates the service image correctly, then a call to 

    concierto push db

will push your image to the docker registry whose URL is provided as the _docker.registry.url_ key in the access.edn file for your deployment.

=== Pre and Post Build

Your service directory can contain pre and post build scripts

    services/
        db/
            pre_build
            Dockerfile
            post_build

If these executable scripts exist (any language with #!/usr/bin/env mylang), Concierto will call them before building the Dockerfile and after. So, for example, if you need to download any resource and tidy up afterwards you may.

The pre and post build scripts are templated before execution from any of the targets resource files, and are run with the service directory.

[TIP]
The concierto template command is very useful within the pre_build/post_build scripts. It allows you to template files easily during build with all the target details available, e.g. in a pre_build

    #!/usr/bin/env sh
    concierto template --in nginx.tmpl --out nginx.conf

=== Aiding Integration

==== Alternate Docker file

You may specify an alternative to Dockerfile on the command line by specifying --build-file, also --no-cache is available, e.g.

    concierto build X --build-file ConciertoDockerfile --no-cache

This is useful when testing and you already have an existing Dockerfile in a service directory and you don't want to tweak it to try out Concierto.

==== Env Files

Existing services will already have environment variables defined, and probably as env files.  Again, you probably don't want to tweak existing files, so by creating a Docker env file within the service directory and referencing it from the role file Concierto will template it and deploy it with the role file.

[source, yaml]
----
version: "3.7"
services:
  api:
    image: "{{services.api.image|registry}}"
    container_name: "api"
    ports:
      - "8080:8080"
    env_file: 
      - api.env
----

Concierto looks for api.env in the root dir of the service. api.env should be an env file but presumably templated, and should exist in the service directory ...

[source,bash]
----
DB_HOST={{cluster.db}}
DB_PORT={{globals.DB_PORT}}
DB_PASSWORD={{globals.DB_PASSWORD}}
MY_HOST={{machine.host}}
----

[TIP]
Using env files has the nice property of syncing Concierto variables with the variables already in use by services.

== Deploy

The Concierto deploy process is simply identifying a set of machines primarily by role, e.g.

    concierto deploy --select role api

where api is the role, and then to restrict the set of machines further by adding, e.g. a cluster

    concierto deploy --select role api cluster us

The best way to test the machines set is simply to list them first ...

    concierto machines --select role api
    concierto machines --select role api cluster us

Further, you may examine what commands are actually to be executed by adding the --dry-run option ...

    concierto machines --select role api --dry-run

A typical deploy of an entire cluster will be in, for example, three separate tiers, thus requiring 3 separate calls to Concierto.

    concierto deploy --select role data cluster us
    concierto deploy --select role api cluster us
    concierto deploy --select role balancer cluster us

and then obviously packaging the sequence in a script, say deploy_cluster defined below ...

    #!/usr/bin/env bash
    concierto deploy --select role data cluster $1
    concierto deploy --select role api cluster $1
    concierto deploy --select role balancer cluster $1

so that you may

    ./deploy_cluster us

Thus, using the simple Concierto calls as plumbing and tieing them together as simple scripts is the envisaged way of using Concierto. 

=== Deploy Process

Concierto loads all target attributes once per run, and iterates through the machines, creating temporary directories on your local machine for each IP. During this process it templates the role files, and environment files with machine and attribute data and deposits an executor script into the same directory. The executor scipt is what is acutally doing the work on the server during the deploy. The executor script is replaceable and templatable (this fact allows you to transfer any attribute for server side processing.)

Then each temporary machine directory is tarred up and scp'd to a unique tar file on the host in question and untarred. Then the embedded executor script is run which determines if docker or podman is installed  on the host, and executes compose on the given yml file as appropriate.

The default executor script is below ...

[source, bash]
----
include::../resource/remote-exec[]
----

[IMPORTANT]
During the deploy process, in addition to the attributes described by the *gather* command, Concierto adds the :machine key with all the details of the machine currently being processed. As can be seen above, that's very important in the executor script.

Finally, all the deploy temporaries are tidied up on your local machine, unless you use the --debug option of deploy which drops the fully templated files into the "debug" directory without uploading and execution.

==== Replacing the Executor script

The remote executor can be replaced by providing a :remote-exec key in the config file that is a path to an execution script. Obviously it has to do what's done above but it could be in any language and have additions. For example, provisioning Babashka on each server with a Babashka executor script would be easy.

[Tip]
Extensions could replace the Executor script.

== Hooks

hooks.clj files can be associated with targets, within them you may define the following hooks as functions:

[cols="2,3"]
|===
|attr-cluster [cluster]
|called once per cluster with the cluster name. Creates the :clusters attribute.

|attr-machine [machine-data cluster-args]
|Called once per machine with all machine-data and the cluster data returned by attr-cluster. Return value is merged with machine-data.

|on-deploy
|called on deploy end
|===

[IMPORTANT]
The hooks.clj file is the primary method of generating dynamic attributes during a deploy. Note the attr-* hooks are evaluated early before deploy or build and you can see their output in concierto gather

In the example below, given a cluster name, the attr-cluster function find the db machine of the cluster and the api machines of the cluster, and creates keys which will be merged into the final attribute set under the :clusters key.

The attr-machine function receives the arguments generated by the attr-cluster function, so they can be employed in any dynamic attribute operations per machine.

[source, clojure]
----
include::../../starter-pack/targets/live/hooks.clj[]
----

Because you may want to share hooks between targets, you may place an hooks.clj file within the targets directory, and those hooks which are not overridden by hooks.clj in a targets/TARGET/hooks.clj file will be executed.

So, if all targets share the same hooks just add one hooks.clj file in the targets directory itself.

== Extensions

New functionality to the command line may be added via Clojure extensions.

Because Concierto is driven from the command line, extensions add themselves to the command line by providing a typical Babashka Cli dispatch function, as the last function that is evaluated in the extend.clj file.

Typical structure:

    application_name
        ...
        extensions
            mycompany
                disksize
                    extend.clj
            3rdparty
                some_ext
                    extend.clj

For example, here's an extension that retrieves disk sizes for a given mount point ...

[source, clojure]
----
include::../../starter-pack/extensions/mycompany/disksize/extend.clj[]
----

As the dispatch-table function is *last* to be evaluated (and returned) to the calling function, the extension is merged into the normal cli


    EXTENSIONS
    ----------
    mycompany disk-size       Find disk usage for db machines

and running it provides the following output:

    concierto mycompany disk-size --select role db --prefix host -- /dev/fuse
    Scenario: cluster, Target: simulation, Engine: podman
    east-db devtmpfs                  4.0M         0      4.0M   0% /dev/fuse
    west-db devtmpfs                  4.0M         0      4.0M   0% /dev/fuse


== Future

* Formalise extension API

* Tests/specs now that dev is settling down.

* Logging of process output.

* Extensions added from the classpath. I'm not that familiar with Java packaging ecosystem so the extensions are probably a bit naive in the greater scheme of things.

* More extension hook points. Hooks will require to be vectors of callbacks and extensions will need to be able to add their own callbacks, e.g. an extension will want to know if a deploy has been successful.

* A server version, Clojure proper, that provides a REST API with client mimicing current api, providing permissioned access to multiple apps, and targets.

* The ability to bake targets or pipe attributes into a deployment or other process could have merit in some situations, e.g. 

    concierto gather | concierto --read-target-from-stdin deploy
    concierto gather --target blah > baked_blah.edn
    concierto --target baked_blah.edn deploy
    
For example, in testing to create invariants to test against, or for performance if the aggregation of target files becomes expensive.

include::funding.adoc[]